{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Comparisons\n",
    "\n",
    "Compare my implementations and Spinning Up versions of RL algos on MuJoCo Gym tasks.\n",
    "https://spinningup.openai.com/en/latest/spinningup/bench.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, sys\n",
    "import pandas as pd\n",
    "import plotnine as pn\n",
    "DATA_DIR = '/Users/kurtsmith/research/spinningup/data'\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_data(task_list, method_list):\n",
    "    output_paths = {}\n",
    "    full_data = {}\n",
    "    for task in task_list:\n",
    "        output_paths[task] = {}\n",
    "        full_data[task] = {}\n",
    "        for method in method_list:\n",
    "            my_path = f'{DATA_DIR}/{task}/{method}'\n",
    "            if os.path.exists(my_path):\n",
    "                output_paths[task][method] = os.listdir(my_path)\n",
    "    for task in task_list:\n",
    "        for method in output_paths[task].keys():\n",
    "            results = []\n",
    "            for run_num in output_paths[task][method]:\n",
    "#                 print(f'{task} {method} {run_num}')\n",
    "                progress = pd.read_table(f'{DATA_DIR}/{task}/{method}/{run_num}/progress.txt')\n",
    "                progress['Minutes'] =  progress['Time']/60.\n",
    "                progress['Hours'] =  progress['Time']/60./60.\n",
    "                progress['task'] = task\n",
    "                progress['method'] = method\n",
    "                progress['run'] = run_num\n",
    "                results.append(progress)\n",
    "            if len(results)>0:\n",
    "                full_data[task][method] = pd.concat(results)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = ['walker2d_v3', 'ant_v3', 'hopper_v3', 'halfcheetah_v3',  'swimmer_v3']\n",
    "method_list = ['su_ddpg', 'ddpg', 'su_td3', 'td3', 'su_sac', 'sac', 'su_ppo', 'ppo']\n",
    "full_data = get_full_data(task_list, method_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_run_returns(df, col=None, task=None, method=None, line=True, points=False):\n",
    "#     if col is None:\n",
    "#         col = 'AverageTestEpRet' if 'AverageTestEpRet' in df.columns else 'AverageEpRet'\n",
    "#     plot = pn.ggplot(df, pn.aes(x='TotalEnvInteracts', y=col))\n",
    "#     if line:\n",
    "#         plot += pn.geom_line(pn.aes(color='run'))\n",
    "#     if points:\n",
    "#         plot += pn.geom_point(pn.aes(color='run'))\n",
    "#     if task is not None:\n",
    "#         plot += pn.labels.ggtitle(f'{task} : {method}')        \n",
    "#     return plot\n",
    "\n",
    "# def plot_smoothed_returns(df, window_size, plot=True, label='smoothed', **kwargs):\n",
    "#     df_smooth = df.groupby('TotalEnvInteracts').mean().reset_index()\n",
    "#     df_smooth = df_smooth.rolling(window_size).mean()\n",
    "#     df_smooth = df_smooth.dropna()\n",
    "#     df_smooth['run'] = label\n",
    "#     if plot:\n",
    "#         return plot_run_returns(df_smooth, **kwargs)\n",
    "#     else:\n",
    "#         return df_smooth\n",
    "\n",
    "# def compare_runs(task, full_data=full_data, **kwargs):\n",
    "#     for method in full_data[task].keys():\n",
    "#         display(plot_run_returns(full_data[task][method], task=task, method=method, **kwargs))\n",
    "\n",
    "# def compare_smoothed_returns(task, full_data=full_data, **kwargs):\n",
    "#     for method in full_data[task].keys():\n",
    "#         if not 'su_' in method:\n",
    "#             df_mine = plot_smoothed_returns(full_data[task][method], 4, plot = False, label='Mine')\n",
    "#             df_su = plot_smoothed_returns(full_data[task]['su_' + method], 4, plot = False, label='Spinning Up')\n",
    "#             display(plot_run_returns(pd.concat((df_mine, df_su)),\n",
    "#                                      task=task, method=method, points=True, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df_orig, x_col = 'TotalEnvInteracts', col='AverageTestEpRet', window_size=None, avg=False, label='run_average'):\n",
    "    \"\"\"Processes data using window averaging, average over runs, and confidence interval \"\"\"\n",
    "    df = df_orig.sort_values(['run', x_col])\n",
    "    if window_size is not None:\n",
    "        df = df.groupby('run').rolling(window_size).mean().dropna().reset_index()\n",
    "    if avg:\n",
    "        df = df.groupby(x_col).agg({col : ['mean', 'sem']}) \n",
    "        df.columns = [col, 'sem']\n",
    "        df = df.reset_index()\n",
    "        df['ymin'] = df[col] - 1.96*df['sem']\n",
    "        df['ymax'] = df[col] + 1.96*df['sem']\n",
    "        df['run'] = label\n",
    "    return df\n",
    "\n",
    "def plot_runs(df, task=None, method=None, x_col = 'TotalEnvInteracts', col='AverageTestEpRet', points=True, **kwargs):\n",
    "    \"\"\"Plot all runs from a df for specified task/method.\"\"\"\n",
    "    if not col in df.columns:\n",
    "        col = col.replace('Test', '')    \n",
    "    df = process_data(df, avg=False, x_col=x_col, col=col, **kwargs)\n",
    "    plot = pn.ggplot(df, pn.aes(x=x_col, y=col, color='run')) + pn.geom_line()\n",
    "    if points:\n",
    "        plot += pn.geom_point()\n",
    "    plot += pn.labels.ggtitle(f'{task} : {method}')   \n",
    "    display(plot)\n",
    "\n",
    "def plot_runs_all_methods(df, task, **kwargs):\n",
    "    \"\"\"Loop over all methods for specified task. Plot all runs for each method.\"\"\"\n",
    "    for method in df[task].keys():\n",
    "        plot_runs(df[task][method], task=task, method=method, **kwargs)\n",
    "\n",
    "def plot_compare_methods(df, task, method, x_col = 'TotalEnvInteracts', col='AverageTestEpRet', **kwargs):\n",
    "    \"\"\"Plot my implementation vs Spinning Up (avg over runs) for a specified task/method.\"\"\"\n",
    "    if not col in df[task][method].columns:\n",
    "        col = col.replace('Test', '')        \n",
    "    df_mine = process_data(df[task][method], avg=True, label='Mine', x_col=x_col, col=col, **kwargs)\n",
    "    df_su = process_data(df[task]['su_' + method], avg=True, label='Spinning Up', x_col=x_col, col=col, **kwargs)\n",
    "    df = pd.concat((df_mine, df_su))\n",
    "    plot = pn.ggplot(df, pn.aes(x=x_col, y=col, color='run')) + pn.geom_point() + pn.geom_line() + pn.geom_errorbar(\n",
    "        pn.aes(ymin='ymin', ymax='ymax'), alpha=0.5)\n",
    "    plot += pn.labels.ggtitle(f'{task} : {method}')   \n",
    "    display(plot)\n",
    "\n",
    "def compare_all_methods(full_data, **kwargs):\n",
    "    \"\"\"Plot my implementation vs Spinning Up for all methods for a specified task.\"\"\"\n",
    "    for method in full_data[task].keys():\n",
    "        if not 'su_' in method:\n",
    "            plot_compare_methods(full_data, task, method, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mean_with_ci(df, window_size, label='smoothed'):\n",
    "#     x_col = 'TotalEnvInteracts'\n",
    "#     col='AverageTestEpRet'\n",
    "#     df_smoothed_runs = df.sort_values(['run', 'Epoch'])\n",
    "#     df_smoothed_runs = df_smoothed_runs.groupby('run').rolling(window_size).mean().dropna().reset_index()\n",
    "# #     df_smoothed_runs = df.rolling(window_size).mean().dropna().reset_index()\n",
    "#     df_avgd_runs = df_smoothed_runs.groupby(x_col).agg({col : ['mean', 'sem']})\n",
    "#     df_avgd_runs.columns = [col, 'sem']\n",
    "#     df_avgd_runs = df_avgd_runs.reset_index()\n",
    "#     df_avgd_runs['ymin'] = df_avgd_runs[col] - 1.96*df_avgd_runs['sem']\n",
    "#     df_avgd_runs['ymax'] = df_avgd_runs[col] + 1.96*df_avgd_runs['sem']\n",
    "#     df_avgd_runs['run'] = label\n",
    "#     return df_avgd_runs\n",
    "\n",
    "# def compare_smoothed_returns(task, full_data=full_data, method='ddpg', ci=True, **kwargs):\n",
    "#     if not 'su_' in method:\n",
    "#         if ci:\n",
    "#             df_mine = get_mean_with_ci(full_data[task][method], 20, label='Mine')\n",
    "#             df_su = get_mean_with_ci(full_data[task]['su_' + method], 20, label='Spinning Up')\n",
    "#             display(pn.ggplot(pd.concat((df_mine, df_su)), \n",
    "#                                         pn.aes(x=x_col, y=col, color='run')) + \n",
    "#                               pn.geom_point() + pn.geom_errorbar(pn.aes(ymin='ymin', ymax='ymax'), alpha=0.5))\n",
    "#         else:\n",
    "#             df_mine = plot_smoothed_returns(full_data[task][method], 4, plot = False, label='Mine')\n",
    "#             df_su = plot_smoothed_returns(full_data[task]['su_' + method], 4, plot = False, label='Spinning Up')\n",
    "#             display(plot_run_returns(pd.concat((df_mine, df_su)),\n",
    "#                                      task=task, method=method, points=True, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Performance on each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'walker2d_v3'\n",
    "window_size = 10\n",
    "compare_all_methods(full_data, window_size=window_size)\n",
    "plot_runs_all_methods(full_data, task, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'ant_v3'\n",
    "window_size = 10\n",
    "compare_all_methods(full_data, window_size=window_size)\n",
    "plot_runs_all_methods(full_data, task, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'hopper_v3'\n",
    "window_size = 10\n",
    "compare_all_methods(full_data, window_size=window_size)\n",
    "plot_runs_all_methods(full_data, task, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'halfcheetah_v3'\n",
    "window_size = 10\n",
    "compare_all_methods(full_data, window_size=window_size)\n",
    "plot_runs_all_methods(full_data, task, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = 'walker2d_v3'\n",
    "# compare_runs(task, points=True)\n",
    "# compare_smoothed_returns(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in task_list:\n",
    "    print('******************************')\n",
    "    print(task)\n",
    "    print('******************************')\n",
    "    plot_runs_all_methods(full_data, task, col='Minutes', points=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = 'walker2d_v3'\n",
    "# col = 'StdEpRet'\n",
    "# compare_runs(task, points=True, col=col)\n",
    "# compare_smoothed_returns(task, col=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = 'walker2d_v3'\n",
    "# compare_runs(task, points=True, full_data=full_data_64)\n",
    "# compare_smoothed_returns(task, full_data=full_data_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "def f2():\n",
    "    global x\n",
    "    x += 2\n",
    "for i in range(3):\n",
    "    f2()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global x\n",
    "def f1():\n",
    "    x = 0\n",
    "    def f2():\n",
    "        global x\n",
    "        x += 2\n",
    "    for i in range(3):\n",
    "        f2()\n",
    "        print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for i in range(500):\n",
    "    agent_target.q = target_update(agent.q, agent_target.q, polyak=polyak)\n",
    "    agent_target.pi = target_update(agent.pi, agent_target.pi, polyak=polyak)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for i in range(500):\n",
    "    for p, p_targ in zip(agent.parameters(), agent_target.parameters()):\n",
    "                    p_targ.data.mul_(polyak)\n",
    "                    p_targ.data.add_((1 - polyak) * p.data)\n",
    "t1 = time.time()\n",
    "print(t1-t0)            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
